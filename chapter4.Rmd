---
title: "chapter3"
author: "RK"
date: "25 November 2018"
output: html_document
---

## Chapter 4

This week, I downloaded the MASS library, and with it, the Boston dataset.

It seems like the Boston dataset has 506 rows (observations) and 14 columns (variables).

```{r, include = FALSE, echo = FALSE}
library(dplyr)
library(ggplot2)
library(tidyr)
library(corrplot)
library(MASS)
```

```{r, , echo = FALSE}
data("Boston")
str(Boston)
dim(Boston)

```

The Boston data describes housing values in the suburbs of Boston. The columns in the data are:

`crim` per capita crime rate by town.
`zn` proportion of residential land zoned for lots over 25,000 sq.ft.
`indus` proportion of non-retail business acres per town.
`chas` Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
`nox` nitrogen oxides concentration (parts per 10 million).
`rm` average number of rooms per dwelling.
`age` proportion of owner-occupied units built prior to 1940.
`dis` weighted mean of distances to five Boston employment centres.
`rad` index of accessibility to radial highways.
`tax` full-value property-tax rate per \$10,000.
`ptratio` pupil-teacher ratio by town.
`black`1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.
`lstat` lower status of the population (percent).
`medv` median value of owner-occupied homes in \$1000s.



For starters, let's explore the data graphically with a correlation matrix.

```{r, echo=FALSE}

# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round(digits = 2)

# print the correlation matrix
cor_matrix

# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex = 0.6)

```



In a corrplot (above), positive correlations are displayed in blue and negative correlations in red color. Color intensity and circle size are proportional to the correlation coefficients.

Therefore, I'll keep an eye out for large dark blue & red circles.

Some notes:
`rad` and `tax` correlate positively (highway accessibility correlates with high tax rate)
`lstat` (lower status) correlates negatively with `medv` (median value of owner-occupied homes)
`age` and `dis` (distance to employment center) have a strong negaive correlation.
`indus` (industry) and `nox` (nitrogen oxide) are positively correlated (little surprise).
And so forth.

However, at the moment, the variables are in different scales. Clustering methods take the assumption that scales are standardized, so let's standardize the data (**(x - mean(x))/sd(x)**).

```{r, echo=FALSE, message=FALSE}
# center and standardize variables
boston_scaled <- scale(Boston)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

# summaries of the scaled variables
summary(boston_scaled)

# class of the boston_scaled object
class(boston_scaled)

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# set seed to 1
set.seed(1)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)


```
As we notice, the variables are now on a comparable scale. In other words, the variables have now been rescaled to have a mean of zero and a standard deviation of one.This means that each case's value indicates it's difference from the mean of the original variable in the number of standard deviations. This is a familiar procedure from e.g. p-value calculation (Z-scores).

I also created a categorical variable of the crime rate in the Boston datasetand dropped the old crime rate variable from the dataset. This is necessary for the clustering we will do later on.

Moreover, I finally divided the dataset to train and test sets. Now 80% of the data belongs to the train set and 20% to the test set.

Now I will fit the Linear Discriminant Analysis to the train data, by using `crime` as a target variable and all other variables as predictors.

```{r, echo=FALSE}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot <- plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)

```

As we notice, LDA1 explains about 95% of variance within the data. We could interpret the coefficients of linear discriminants to see how the specific 14 variables cluster the data, but it's much nicer to draw it on a plot. So above you also see a biplot doing just this.

Above you see an LDI biplot with an arrow function. We can interpret from this picture the strenght of specific variables have in clustering the data (and to which direction). For example, `rad` seems to cluster the data into the high-category.

Next I will predict test data with trained model.

```{r, echo=FALSE}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)

```

It looks like our LDA model is particularly competent in predicting high crime rate (in fact, it gets 19 out of 19 "guesses" correct). This is, for practical purposes, quite good. It does a less good job at predicting med_high and mew_low crime rates, but it's alright at predicting low crime rates.

Next, I will move on to k-means clustering. Let's reload the data and standardize it (much like above):

```{r, echo=FALSE}
data("Boston")
# center and standardize variables
boston_scaled2 <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled2)

# class of the boston_scaled object
class(boston_scaled2)

```

Now count euclidean and manhattan distances.

```{r, echo=FALSE}
# euclidean distance matrix
dist_eu <- dist(boston_scaled2)

# look at the summary of the distances
summary(dist_eu)

# manhattan distance matrix
dist_man <- dist(boston_scaled2, method = 'manhattan')

# look at the summary of the distances
summary(dist_man)

```

There's nothing surprising here: manhattan distances seem longer, but the in fact the relations between euclidean and manhattan distances are more or less similar.

Now I'll do some k-means clustering (with scaled data).

First I'll run a clustering process with three centers.

```{r, echo=FALSE}
# k-means clustering
km <-kmeans(boston_scaled2, centers = 3)

# plot the Boston dataset with clusters
pairs(boston_scaled2, col = km$cluster)

```

But what I really want to do is find out the optimal number of clusters. So let's try all numbers of clusters from 1 to 10 and visualise the reslts as an elbow plot. (I use a random seed of 123.)

```{r, echo = FALSE}
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled2, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
```

As we notice, the "elbow" of this plot is when k = 2. This is the most efficient way to cluster the data (we are trying to minimize variance as efficiently as possible here; theoretically as k reaches the number of variables, this variance reaches 0).

Therefore, I conclude that the optimal number of centers is two. So finally, I will visualise these clusters.

```{r, echo = FALSE}
# k-means clustering
km <-kmeans(boston_scaled2, centers = 2)

# plot the Boston dataset with clusters
pairs(boston_scaled2, col = km$cluster)

```

And there you have it. There is a lot (!) to interpret here, but to get to the nitty-gritty: the k-means algorithm seems to do a decent job at clustering. Along the variables, there's some amount of overlap (e.g. pairs `rm` and `nox`; `dis` and `nox`; etc.). On the other hand, some pairs are very neatly clustered (e.g. `tax` and `crime`; `black` and `zn`). So I think the k-means algorithm is doing a decent job here.

That's all for this week. Thanks for reading!

